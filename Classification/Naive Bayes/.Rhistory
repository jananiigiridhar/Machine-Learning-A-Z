library(corrgram)
corrgram(Boston)
corrgram(Boston,order = T)
corrgram(Boston,order = T,upper.panel = panel.pie)
#take a look at ggplot
install.packages("ggplot")
library(ggplot2)
#programming and functions
#Numeric Prediction
NumericPred <- read.csv("~/MSBAPM/Data Analytics using R/Session 3/data/NumericPred.csv")
View(NumericPred)
colnames(NumericPred) = c("target","model1","model2")
a = NumericPred$target
m = NumericPred$model1
#useful metrics to compare model performance - Rsquare, MAD, MAPE, MSE, MPSE
metrics = c(MAD = 0,MSE = 0,MAPE = 0,MPSE = 0, R2 = 0,TMAD = 0)
metrics
metrics["MAD"] = mean(abs(a-m)) #Mean absolute difference
metrics["MSE"] = mean((a-m)^2) #Mean squared Error
metrics["MAPE"] = mean(abs((a-m)/a)) #Mean absolute percent error
metrics["MPSE"] = mean(((a-m)/a)^2) #Mean percent square error
metrics["TMAD"] = mean(abs(a-m),trim = 0.05) #Trimmed Mean percent error
#R2 value
SST = sum((a - mean(a))^2)
SSE = sum((a - m)^2)
metrics["R2"] = 1 - (SSE/SST)
metrics
#Functions
nummetrics = function(a,m){
#useful metrics to compare model performance - Rsquare, MAD, MAPE, MSE, MPSE
metrics = c(MAD = 0,MSE = 0,MAPE = 0,MPSE = 0, R2 = 0,TMAD = 0)
metrics["MAD"] = mean(abs(a-m)) #Mean absolute difference
metrics["MSE"] = mean((a-m)^2) #Mean squared Error
metrics["MAPE"] = mean(abs((a-m)/a)) #Mean absolute percent error
metrics["MPSE"] = mean(((a-m)/a)^2) #Mean percent square error
metrics["TMAD"] = mean(abs(a-m),trim = 0.05) #Trimmed Mean percent error
#R2 value
SST = sum((a - mean(a))^2)
SSE = sum((a - m)^2)
metrics["R2"] = 1 - (SSE/SST)
return(metrics)
}
m2 = NumericPred$model2
nummetrics(a,m2)
nummetrics(a,m)
#saving function
dump('nummetrics',file = "MyModelMetrics.R")
#Ensemble model
ensemble = lm(NumericPred$target ~ NumericPred$model1+NumericPred$model2)
summary(ensemble)
#Getting the predicted value from ensemble model in a new column
NumericPred$model3 = -1.11362+0.29936*NumericPred$model1+0.75044*NumericPred$model2
#comparing the model 3 performance
nummetrics(a,NumericPred$model3)
#Visual look
plot(NumericPred$target,NumericPred$model3)
lines(NumericPred$target,NumericPred$target, col = "Red",lwd = 2)
#Take a look at points when they are more than 5 away from the line
sub1 = NumericPred[abs(NumericPred$target - NumericPred$model3) > 5, ]
points(sub1$target,sub1$model3,pch = 19, col = "blue")
#cost of error
NumericPred$cost = ifelse(abs(NumericPred$target - NumericPred$model3) > 5,
abs(NumericPred$target - NumericPred$model3) * 2,
0)
sum(NumericPred$cost)
#compare the cost with the baseline (ie when u don't have a model)
NumericPred$baseline = mean(NumericPred$target)
NumericPred$baseline
NumericPred$basecost = ifelse(abs(NumericPred$target - NumericPred$baseline) > 5,
abs(NumericPred$target - NumericPred$baseline) * 2,
0)
sum(NumericPred$basecost)
#Binary Prediction
BinaryPred = read.csv("C:/Users/Vinodh Mohan/Documents/MSBAPM/Data Analytics using R/Session 3/data/BinaryPred.csv")
colnames(BinaryPred) = c("target","model1","model2")
a = BinaryPred$target
m = BinaryPred$model1
metrics = c(LL=0,AIC=0,BIC=0,R2=0)
metrics['LL'] = sum(ifelse(a==1,log(m),log(1-m)))
metrics
a = BinaryPred$target
m = BinaryPred$model1
k = 10
binmetrics = function(a,m,k){
metrics = c(LL=0,AIC=0,BIC=0,R2=0)
metrics['LL'] = sum(ifelse(a==1,log(m),log(1-m)))
metrics['AIC'] = -2*metrics['LL']+2*k
metrics['BIC'] = -2*metrics['LL']+2*K*log(length(a))
SST = sum((a-mean(a))^2)
SSE = sum((a-m)^2)
metrics['R2'] = 1 - (SSE/SST)
return(metrics)
}
binmetrics = function(a,m,k=10){
metrics = c(LL=0,AIC=0,BIC=0,R2=0)
metrics['LL'] = sum(ifelse(a==1,log(m),log(1-m)))
metrics['AIC'] = -2*metrics['LL']+2*k
metrics['BIC'] = -2*metrics['LL']+2*K*log(length(a))
SST = sum((a-mean(a))^2)
SSE = sum((a-m)^2)
metrics['R2'] = 1 - (SSE/SST)
return(metrics)
}
binmetrics(a=BinaryPred$target,BinaryPred$model1)
binmetrics = function(a,m,k=10){
metrics = c(LL=0,AIC=0,BIC=0,R2=0)
metrics['LL'] = sum(ifelse(a==1,log(m),log(1-m)))
metrics['AIC'] = -2*metrics['LL']+2*k
metrics['BIC'] = -2*metrics['LL']+2*K*log(length(a))
SST = sum((a-mean(a))^2)
SSE = sum((a-m)^2)
metrics['R2'] = 1 - (SSE/SST)
return(metrics)
}
binmetrics(a=BinaryPred$target,BinaryPred$model1)
binmetrics(a=BinaryPred$target,m = BinaryPred$model1)
binmetrics(a=BinaryPred$target,m = BinaryPred$model2)
a = BinaryPred$target
m = BinaryPred$model1
k = 10
binmetrics = function(a,m,k=10){
metrics = c(LL=0,AIC=0,BIC=0,R2=0)
metrics['LL'] = sum(ifelse(a==1,log(m),log(1-m)))
metrics['AIC'] = -2*metrics['LL']+2*k
metrics['BIC'] = -2*metrics['LL']+2*K*log(length(a))
SST = sum((a-mean(a))^2)
SSE = sum((a-m)^2)
metrics['R2'] = 1 - (SSE/SST)
return(metrics)
}
binmetrics = function(a,m,k=10){
metrics = c(LL=0,AIC=0,BIC=0,R2=0)
metrics['LL'] = sum(ifelse(a==1,log(m),log(1-m)))
metrics['AIC'] = -2*metrics['LL']+2*k
metrics['BIC'] = -2*metrics['LL']+2*k*log(length(a))
SST = sum((a-mean(a))^2)
SSE = sum((a-m)^2)
metrics['R2'] = 1 - (SSE/SST)
return(metrics)
}
binmetrics(a=BinaryPred$target,m = BinaryPred$model1)
binmetrics(a=BinaryPred$target,m = BinaryPred$model2)
p = 0.7
mnew = ifelse(m>p,1,0)
contab = table(factor(mnew),factor(a))
contab
contab
vec = as.vector(contab)
vec
vec = c(p,vec)
vec
cutoffs
increment = 0.05
cutoffs = seq(min(m),max(m),by = increment)
cutoffs
mresult = data.frame()
mresult = data.frame()
for(p in cutoffs){
mnew = ifelse(m>p,1,0)
contab = table(factor(mnew),factor(a))
vec = as.vector(contab)
vec = c(p,vec)
mresult = rbind(mresult,vec)
}
View(mresult)
colnames(mresult) = c('CutOff',TN','FP','FN','TP')
colnames(mresult) = c('CutOff','TN','FP','FN','TP')
colnames(mresult) = c('CutOff','TN','FP','FN','TP')
View(mresult)
binresult = function(a,m,increment = 0.05){
cutoffs = seq(min(m),max(m),by = increment)
mresult = data.frame()
for(p in cutoffs){
mnew = ifelse(m>p,1,0)
contab = table(factor(mnew),factor(a))
vec = as.vector(contab)
vec = c(p,vec)
mresult = rbind(mresult,vec)
}
colnames(mresult) = c('CutOff','TN','FP','FN','TP')
return(mresult)
}
binresult(a = BinaryPred$target,BinaryPred$model2)
setwd("~/R_Workspace/DataAnalyticsUsingR")
z = binresult(a = BinaryPred$target,BinaryPred$model2)
plot(x1,y1,type = 'l')
x1 =z$FP/max(z$FP)
y1 = z$TP/max(z$TP)
plot(x1,y1,type = 'l')
AUC = mean(y1)
AUC
text(0.5,0.9,paste("AUC = ",AUC))
text(0.5,0.9,paste("AUC = ",round(AUC)))
plot(x1,y1,type = 'l')
text(0.5,0.9,paste("AUC = ",round(AUC)))
plot(x1,y1,type = 'l')
text(0.5,0.9,paste("AUC = ",round(AUC,2)))
v = c('H','T')
p = c('0.5','0.5')
v = c('H','T')
p = c('0.5','0.5')
sample(v,100,prob = p)
sample(v,100,prob = p, replace = TRUE)
sample(x = v,10,prob = p, replace = TRUE)
sample(x = v,10,prob = p, replace = TRUE)
v = seq(1,6)
p = repeat(1/6,6)
p = rep(1/6,6)
sample(x = v,size = 3,prob = p,replace = TRUE)
dnorm(x = 2, mean = 12, sd = 5)
pnorm(x = 2, mean = 12, sd = 5)
pnorm(q = 2, mean = 12, sd = 5)
qnorm(p = 0.75,mean = 12, sd = 5)
rnorm(n = 10,mean = 12,sd = 5)
dnorm(x = 12, mean = 12, sd = 5)
x = rnorm(n=10000,mean = 5, sd = 2)
plot(density(x))
dnorm(x = 5, mean = 5, sd = 2) #max y value
dnorm(x = 12, mean = 12, sd = 5) #max y value
x = rpois(10000,lambda = 4)
plot(density(x))
x = rlnorm(10000,meanlog = 4,sdlog = 1)
plot(density(x))
r1 = rnorm(2000)
r2 = rnorm(2000)
u1 = ruinf(2000)
u2 = runif(2000)
u1 = ruinf(2000)
u2 = runif(2000)
u1 = runif(2000)
plot(density(r1))
plot(density(u1))
x = r1+r2
plot(density(x))
shapiro.test(x)
x = r1*r2
plot(density(x))
shapiro.test(x)
x = r1^2
plot(density(x))
shapiro.test(x)
x = log(r1)
plot(density(x))
shapiro.test(x)
x = r1+u1
plot(density(x))
shapiro.test(x)
?shapiro.test
x = u1+u2
plot(density(x))
shapiro.test(x)
x = r1+r2+u1+u2
plot(density(x))
shapiro.test(x)
x = u1+u2+u3+u4+u5+u6
u3 = runif(2000)
u4 = runif(2000)
u5 = runif(2000)
u6 = runif(2000)
x = u1+u2+u3+u4+u5+u6
plot(density(x))
shapiro.test(x)
x = u1+u2 #triangular
plot(density(x))
shapiro.test(x)
x = r1+r2
plot(density(x))
shapiro.test(x)
x = r1*r2
plot(density(x))
shapiro.test(x)
x = r1^2
plot(density(x))
shapiro.test(x)
x = r1+u1
plot(density(x))
shapiro.test(x)
r1 = rnorm(1000,5,4)
u1 = runif(1000,3,7)
x = r1+u1
r1 = rnorm(50000,5,4)
u1 = runif(50000,3,7)
x = r1+u1
pnorm(q = 10,mean(x),sd(x))
pnorm(q = 10,mean(r1),sd(r1))
pnorm(q = 10,mean(u1),sd(u1))
p1*p2
p1 = pnorm(q = 10,mean(r1),sd(r1))
p2 = pnorm(q = 10,mean(u1),sd(u1))
p1*p2
#two tasks t1(time for task 1), t2(time for task 2), one after another
t1 = rnorm(50000,5,4)
t2 = runif(50000,3,7)
t = t1+t2
pnorm(q = 10,mean(t),sd(t))
#two tasks t1(time for task 1),t2(time for task 2), together
t1 = rnorm(50000,5,4)
t2 = runif(50000,3,7)
p1 = pnorm(q = 10,mean(t1),sd(t1))
p2 = pnorm(q = 10,mean(t2),sd(t2))
p1*p2
s1 = t[t <10]
length(s1)/length(t)
t1 = rnorm(50000,5,4)
t2 = runif(50000,3,7)
t = max(t1,t2)
s1 = t[t<10]
length(s1)/length(t)
length(s1)/length(t)
t = ifelse(t1 > t2,t1,t2)
s1 = t[t<10]
length(s1)/length(t)
plot(density(t))
plot(density(t))
#two tasks t1(time for task 1), t2(time for task 2), one after another
t1 = rnorm(50000,5,4)
t2 = runif(50000,3,7)
t = t1+t2
#pnorm(q = 10,mean(t),sd(t))
s1 = t[t<10]
length(s1)/length(t)
plot(density(t))
#two tasks t1(time for task 1),t2(time for task 2), together
t1 = rnorm(50000,5,4)
t2 = runif(50000,3,7)
# p1 = pnorm(q = 10,mean(t1),sd(t1))
# p2 = pnorm(q = 10,mean(t2),sd(t2))
# p1*p2
t = ifelse(t1 > t2,t1,t2)
s1 = t[t<10]
length(s1)/length(t)
plot(density(t))
median(t)
median(t)
#two tasks t1(time for task 1), t2(time for task 2), one after another
t1 = rnorm(50000,5,4)
t2 = runif(50000,3,7)
t = t1+t2
#pnorm(q = 10,mean(t),sd(t))
s1 = t[t<10]
length(s1)/length(t)
plot(density(t))
median(t)
#two tasks t1(time for task 1),t2(time for task 2), together
t1 = rnorm(50000,5,4)
t2 = runif(50000,3,7)
# p1 = pnorm(q = 10,mean(t1),sd(t1))
# p2 = pnorm(q = 10,mean(t2),sd(t2))
# p1*p2
t = ifelse(t1 > t2,t1,t2)
s1 = t[t<10]
length(s1)/length(t)
plot(density(t))
median(t)
x = rnorm(100,mean = 12, sd = 5)
mean(x)
sd(x)
x = rnorm(100,mean = 12, sd = 5)
mean(x)
sd(x)
x = rnorm(100,mean = 12, sd = 5)
mean(x)
sd(x)
x = rnorm(100,mean = 12, sd = 5)
mean(x)
sd(x)
x = rnorm(100,mean = 12, sd = 5)
mean(x)
sd(x)
x = rnorm(100,mean = 12, sd = 5)
mean(x)
sd(x)
x = rnorm(100,mean = 12, sd = 5)
mean(x)
sd(x)
x = rnorm(100,mean = 12, sd = 5)
mean(x)
sd(x)
x = rnorm(100,mean = 12, sd = 5)
mean(x)
sd(x)
x = rnorm(100,mean = 12, sd = 5)
mean(x)
sd(x)
x = rnorm(100,mean = 12, sd = 5)
mean(x)
sd(x)
x = rnorm(100,mean = 12, sd = 5)
mean(x)
sd(x)
x = rnorm(100,mean = 12, sd = 5)
mean(x)
sd(x)
setwd("~/MSBAPM/Machine Learning A-Z udemy/Classification/SVM")
library(e1071)
Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
classifier = svm(formula = Purchased ~ .,data = training_set,type = 'C-classification',kernel = 'linear')
y_pred = predict(classifier, newdata = test_set[-3])
cm = table(test_set[, 3], y_pred)
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'SVM (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'SVM (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
library(e1071)
library(e1071)
classifier = svm(formula = Purchases ~ .,data = training_set,type = 'C-classification',kernel = 'radial basis')
library(e1071)
classifier = svm(formula = Purchased ~ .,data = training_set,type = 'C-classification',kernel = 'radial basis')
classifier = svm(formula = Purchased ~ .,data = training_set,type = 'C-classification',kernel = 'radial')
y_pred = predict(classifier, newdata = test_set[-3])
cm = table(test_set[, 3], y_pred)
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'Kernel SVM (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
setwd("~/MSBAPM/Machine Learning A-Z udemy/Classification/Naive Bayes")
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
library(e1071)
classifier = naiveBayes(x = training_set[-3],y = training_set$Purchased)
y_pred = predict(classifier, newdata = test_set[-3])
cm = table(test_set[, 3], y_pred)
# Visualising the Training set results
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'Naive Bayes (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
